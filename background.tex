This chapter elaborates on relevant concepts revolving around energy-aware
resource management in edge and fog computing. Section
\ref{sec:edge_and_fog_computing_paradigms} introduces the motivation and
explains the necessity for edge and fog computing paradigms. Section
\ref{sec:testbeds_in_edge_and_fog_computing_environments} focuses on the
development process of edge and fog infrastructure and outlines the essential
use of testbeds. Section \ref{sec:dynamic_resource_management} presents common
techniques in dynamic resource management whilst focusing on energy proportional
computing. Since the renewable energy source of the hardware testbed is not able
to supply uninterrupted power, section
\ref{sec:energy-aware_resource_management} outlines specific techniques used in
energy-aware resource management. Dynamic voltage and frequency scaling will be
the primary technique used in the hardware testbed and is therefore elaborated
on further in section \ref{sec:dynamic_voltage_and_frequency_scaling}.

\section{Edge and Fog Computing Paradigms}
\label{sec:edge_and_fog_computing_paradigms}

In 2012 Xia et al. defines the Internet of Things as the networked
interconnection of everyday objects, which are often equipped with ubiquitous
intelligence \cite{xia2012}. Since then, IoT devices have spread to all aspects
of our everyday lives. Cisco estimates that by 2023 there will be 14.7 billion
machine-to-machine connections, more than a 140\% increase to 2018
\cite{cisco2020}. The rise of IoT results in a significant increase of devices
connected to the internet and may, with its current rate of development, soon
grow to a point, that our current network structure cannot manage. In specific,
major challenges that are posed by traditional cloud-centric IoT architectures
include the excess of bandwidth availability due to the increasingly large and
high-frequent rate of data being produced by IoT devices, and high end-to-end
latency along with unstable, intermittent network connectivity due to
potentially large physical distance \cite{buyya2019}.

As an alternative to cloud-centric architectures, computing resources can be
brought closer to end-devices. Using these resources, even partially, for data
processing would reduce the amount of data sent to the cloud. With the physical
proximity of provided services, latency for end-devices is highly lowered, cloud
servers are relieved due to less network traffic and network throughput is
consequently raised, favoring both end-devices and cloud servers. With these
alternative paradigms, fog and edge computing are introduced.

The terms \emph{fog} and \emph{edge} computing are often used inconsistently
throughout current literature. The research field is still rather young and
allows for multiple interpretations. In the context of this thesis, the
distinction from Iorga et al., from the National Institute of Standards and
Technology, will be used. Iorga et al.\ define fog computing as a layered model
for enabling ubiquitous access to a shared continuum of scalable computing
resource, whereas edge computing describes the layer of end-devices that are
used to do limited local computing or sensor metering \cite{iorga2018}. A
concurrent example for this interpretation of the terminology is presented by
Atos' BullSequana edge computing servers, utilized for visual quality control in
production lines. Possible defects are detected locally and may subsequently be
transmitted to a nearby fog server for further processing \cite{atos2020}.

\section{Testbeds in Edge and Fog Computing Environments}
\label{sec:testbeds_in_edge_and_fog_computing_environments}

\section{Dynamic Resource Management}
\label{sec:dynamic_resource_management}

\subsection{Energy-Aware Resource Management}
\label{sec:energy-aware_resource_management}

\subsection{Dynamic Voltage and Frequency Scaling}
\label{sec:dynamic_voltage_and_frequency_scaling}

